{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework of Ch3. Malware Behavior Log Classification\n",
    "----\n",
    "This is the homework snippet of TU-ETP-AD1062 Machine Learning Fundamentals.\n",
    "\n",
    "For more information, please refer to:\n",
    "https://sites.google.com/view/tu-ad1062-mlfundamentals/\n",
    "\n",
    "> You do NOT have to build up from nothing, please try your best for the following parts:\n",
    "> - **Your task: HW3.3.1.**\n",
    "> - **Your task: HW3.3.2.**\n",
    "> - **Your task: HW3.4.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.1. Import Packages\n",
    "----\n",
    "- Data pre-processing:\n",
    "    - `pandas`: Used for CSV reading\n",
    "    - `os`: Used for path join\n",
    "    - `sklearn.preprocessing.LabelEncoder`: Convert string-based labels into numeric labels\n",
    "- Classifier training and predicting:\n",
    "    - `lightgbm`: Gradient boosting (Ch.3)\n",
    "    - `sklearn.svm.SVC`: Support Vector Machine (Ch.2, Ch.3)\n",
    "    - `sklearn.neural_network.MLPClassifier`: Multi-Layer Perceptron (Ch.3)\n",
    "- Performance evaluation:\n",
    "    - `sklearn.model_selection.cross_validate`: **Automatically** divide your data into training and validation set for k-times, construct classifier and compute the scores, which is for k-fold cross-validation\n",
    "    - `sklearn.metrics.zero_one_loss`: Used for accuracy evaluation\n",
    "    - `sklearn.model_selection.train_test_split`: Divide your data into training and validation set for once, then feed into classifier by yourself, observing the score and confusion matrix\n",
    "    - `mlfund.plot.PlotMetric`: plot confusion matrix (provided by this repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from mlfund.plot import PlotMetric\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.2. Data pre-processing\n",
    "----\n",
    "The code snippet is used to:\n",
    "1. Read CSV files,\n",
    "2. Convert the required part into `numpy.ndarray` for scikit-learn training and predicting, and\n",
    "3. Convert the string labels into numeric labels by `sklearn.preprocessing.LabelEncoder`, i.e.,:\n",
    "    - `PWS:Win32/Fareit`: 0\n",
    "    - `Trojan:HTML/Brocoiner`: 1\n",
    "    - `Trojan:O97M/Obfuse`: 2\n",
    "    - ...\n",
    "    - `VirTool:Win32/VBInject`: 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.2.1. Read CSV Files by Pandas\n",
    "----\n",
    "Here we simply use `pandas.read_csv` for the csv reading. Notice that:\n",
    "- The first column `id` should be ignored, therefore we accessed the values from the 1-st column instead of 0-th column (i.e., using `.values[:, 1:]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "df_train_feature = pd.read_csv(os.path.join('data', 'hw03_dataset.train.feature.csv'))\n",
    "df_train_label = pd.read_csv(os.path.join('data', 'hw03_dataset.train.label.csv'))\n",
    "\n",
    "X_train = df_train_feature.values[:, 1:]\n",
    "y_train_str = df_train_label.values[:, 1:].reshape(len(df_train_label.values[:, 1:]))\n",
    "\n",
    "\n",
    "# Testing test\n",
    "df_test_feature = pd.read_csv(os.path.join('data', 'hw03_dataset.test.feature.csv'))\n",
    "X_test = df_test_feature.values[:, 1:]\n",
    "\n",
    "display(df_train_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.2.2. Convert String Label to Numeric Labels\n",
    "----\n",
    "Use `LabelEncoder` to convert your string lables into `0`, `1`, `2`, ..., and `19`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train_str)\n",
    "\n",
    "y_train = label_encoder.transform(y_train_str)\n",
    "\n",
    "display( [ (idx, label) for idx, label in enumerate(label_encoder.classes_) ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.3. Construct your Classifier\n",
    "----\n",
    "Build your classifier, with parameters fine-tuned.\n",
    "\n",
    "> **Your task: HW3.3.1.**  \n",
    "> Training and Predicting for only once, keep adjusting your `create_clf`, and making sure the parameter is not too bad  \n",
    "> Here we leverage the `train_test_split`, divide your training data `X_train` and `y_train` into:\n",
    "> - 80% `X1`, `y1`, as training set in this round\n",
    "> - 20% `X2`, `y2`, as testing set in this round\n",
    ">\n",
    "> In this round, you're able to observe the confusion matrix, and you're able to check if data from each class is well-classified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clf():\n",
    "clfSVC = sklearn.svm.SVC(C=10, kernel='rbf')",
    "clfSVC.fit(X_train, y_train)",

"y_test_predict = clfSVC.predict(X_test)",

print("Training data:")
print("#SV = %d" % len(clfSVC.support_vectors_))
plot = Plot2D()
plot.scatter(X_train, y_train)
plot.scatterCSVC(clfSVC)
plot.classifierContour(X_train, y_train, clfSVC)
plot.show()

print("Testing data:")
print("MCE = %2.3f" % sklearn.metrics.zero_one_loss(y_test, y_test_predict))
plot = Plot2D()
plot.scatter(X_test, y_test)
plot.classifierContour(X_test, y_test, clfSVC)
plot.show()
    "    # You can use:\n",
    "    #     sklearn.svm.SVC,\n",
    "    #     sklearn.svm.LinearSVC,\n",
    "    #     sklearn.neural_network.MLPClassifier,\n",
    "    #     sklearn.ensemble.GradientBoostingClassifier\n",
    "    #     lightgbm.LGBMClassifier\n",
    "    #     ...\n",
    "    # Or any classifier you found!\n",
    "    # Remember to fine-tune the model parameters\n",
    "    \n",
    "    return LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, X2, y1, y2 = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_clf()\n",
    "model.fit(X1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_predict = model.predict(X2)\n",
    "\n",
    "# Error rate\n",
    "err_01loss = zero_one_loss(y2, y2_predict)\n",
    "print('Error rate = %2.3f' % err_01loss)\n",
    "\n",
    "# Confusion matrix of prediction\n",
    "plot_conf_mat = PlotMetric()\n",
    "plot_conf_mat.set_labels(label_encoder.classes_.tolist())\n",
    "plot_conf_mat.confusion_matrix(y2, y2_predict, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Your task: HW3.3.2.**  \n",
    "> Now you already have a classifier, with the parameter fine-tuned in **HW3.3.1**.\n",
    "> Your model shoud accept more challenges! Lets conduct **cross validation**, which is mentioned in ch.1.:\n",
    "> ![Cross Validation](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)\n",
    ">\n",
    "> If performance is not good, or big differences between different rounds, your model might be over-fitting with particular training/testing set. Keep find-tune your `create_clf` and make your model stable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(create_clf(), X_train, y_train, cv=10, n_jobs=8)\n",
    "display(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.4. Submit to Kaggle InClass\n",
    "----\n",
    "Finally, you should have your classifier model fine-tuned. Now:\n",
    "\n",
    "> **Your task: HW3.4.**\n",
    "> 1. Training with full data set `X_train` with the model created by `create_clf`,\n",
    "> 2. Predict the **unknown** testing data `X_test` by the trained model, then\n",
    "> 3. Submit your result to Kaggle\n",
    "\n",
    "**Notice: You got only 2 chances to submit your result every day, which means you should fine-tune your model by cross-validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and train\n",
    "model = create_clf()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the testing data\n",
    "y_test_predict = model.predict(X_test)\n",
    "y_test_predict_str = label_encoder.inverse_transform(y_test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you submit\n",
    "----\n",
    "Please join the homework 3 competition by **using the Email ended with \\@trendmicro.com as your Kaggle InClass team name**.\n",
    "\n",
    "Type your Email in the variable `my_trendmicro_email_which_is_also_my_team_name` to make sure you've already read this paragraph, then the following code snippet will help you to generate the csv file for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trendmicro_email_which_is_also_my_team_name = ''\n",
    "\n",
    "\n",
    "import re\n",
    "assert re.match(r\"[^@]+@trendmicro.com\", my_trendmicro_email_which_is_also_my_team_name), \"Please read the instruction above paragraph carefully\"\n",
    "\n",
    "target_path = 'data/hw03.result.csv'\n",
    "df_test_label = pd.DataFrame({'id': df_test_feature['id'], 'label': y_test_predict_str})\n",
    "df_test_label.to_csv(target_path, index=False)\n",
    "\n",
    "print('Congratulation! Please submit your result \\'%s\\' to https://www.kaggle.com/t/0415e3e953e54638b617fcd0bc5c04bd' % target_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
